# Embeddings

The core idea of Natural Language Processing (NLP) is to understand the semantics of a word and in extension that of larger units such as document , phrase, sentence, paragraph, collection,â€¦ In order to teach machines to gain a deep understanding of words and concepts, we need to define a representation which they can operate on. As per the definition, Word embedding is the collective name for a set of language modeling and feature learning techniques in natural language processing (NLP) where words or phrases from the vocabulary are mapped to vectors of real numbers. The term word2vec literally translates to word to vector. One Hot Encoding, TF-IDF, Word2Vec, FastText , Bert are frequently used Word Embedding methods. One of these techniques is preferred and used according to
the status, size and purpose of processing the data.


In this project, the main goal is to explore different ways of embeddings and what makes an embedding better than another one. So we will compare word embedding techniques (TF-IDF, Word2Vec and Bert) and propose the best approach out of it. Also we will compare sentence embedding techniques (Doc2Vec, Sentence-Bert and Universal Sentence Encoding) and propose the best approach.
